{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alex Jones (alexander.g.jones.23@dartmouth.edu) <br>\n",
    "March 15, 2022 <br>\n",
    "LING 28 (Rolando Coto-Solano), Winter 2022 <br>\n",
    "Final Project\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains code for performing the web crawling procedure: extracting monolingual Kalaallisut and Danish sentences from multilingual websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hdBteXDEflcD",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clean-text in /Users/sheilaflaherty/miniconda3/lib/python3.9/site-packages (0.6.0)\n",
      "Requirement already satisfied: unidecode in /Users/sheilaflaherty/miniconda3/lib/python3.9/site-packages (1.3.4)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in /Users/sheilaflaherty/miniconda3/lib/python3.9/site-packages (from clean-text) (6.1.1)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in /Users/sheilaflaherty/miniconda3/lib/python3.9/site-packages (from clean-text) (1.7.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/sheilaflaherty/miniconda3/lib/python3.9/site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.5)\n",
      "\u001b[33mWARNING: Error parsing requirements for nltk: [Errno 2] No such file or directory: '/Users/sheilaflaherty/miniconda3/lib/python3.9/site-packages/nltk-3.5.dist-info/METADATA'\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/Users/sheilaflaherty/miniconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install clean-text unidecode\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from cleantext import clean\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zRne_KE-RE_c"
   },
   "outputs": [],
   "source": [
    "# Maps domains to language-specific subdomains, as well as the regex keys for finding relative links associated with those subdomains\n",
    "domain_dict = {'https://greenland-travel.gl':         {'lang_codes': ['', '/kl/'], \n",
    "                                                       'regex_keys': [r'/.*', r'\\/kl\\/.*'],\n",
    "                                                       'file_names': ['gtav_da.txt', 'gtav_kl.txt']},\n",
    "               \n",
    "                'http://www.ral.gl':                  {'lang_codes': [''], \n",
    "                                                       'regex_keys': [r'\\/.*'],\n",
    "                                                       'file_names': ['ral_kl.txt']},\n",
    "               \n",
    "                'http://www.ral.dk':                  {'lang_codes': [''], \n",
    "                                                       'regex_keys': [r'\\/.*'],\n",
    "                                                       'file_names': ['ral_da.txt']},\n",
    "               \n",
    "                'http://www.royalarcticline.com':     {'lang_codes': [''], \n",
    "                                                       'regex_keys': [r'\\/.*'],\n",
    "                                                       'file_names': ['ral_en.txt']},\n",
    "               \n",
    "                'https://ina.gl':                     {'lang_codes': ['/?lang=kl', '/?lang=da', '/?lang=en'], \n",
    "                                                       'regex_keys': [r'\\/\\?lang=kl\\/.*', r'\\/\\?lang=da\\/.*', r'\\/\\?lang=en\\/.*'],\n",
    "                                                       'file_names': ['ina_kl.txt', 'ina_da.txt', 'ina_en.txt']},\n",
    "               \n",
    "                'https://www.banken.gl':              {'lang_codes': ['/gl', '/da/', '/en/'], \n",
    "                                                       'regex_keys': [r'\\/gl\\/.*', r'\\/da\\/.*', r'\\/en\\/.*'],\n",
    "                                                       'file_names': ['banken_kl.txt', 'banken_da.txt', 'banken_en.txt']},\n",
    "               \n",
    "                'https://brugseni.gl':                {'lang_codes': ['', '/gl/'], \n",
    "                                                       'regex_keys': [r'\\/.*', r'\\/gl\\/.*'],\n",
    "                                                       'file_names': ['brug_da.txt', 'brug_kl.txt']},\n",
    "               \n",
    "                'https://diskolineexplorer.com':      {'lang_codes': ['/kl/', '', '/en/'], \n",
    "                                                       'regex_keys': [r'\\/kl\\/.*', r'\\/.*', r'\\/en\\/.*'],\n",
    "                                                       'file_names': ['disko_kl.txt', 'disko_da.txt', 'disko_en.txt']},\n",
    "               \n",
    "                'https://www.mit.gl':                 {'lang_codes': ['', '/en/', '/gl/'], \n",
    "                                                       'regex_keys': [r'\\/.*', r'\\/en\\/.*', r'\\/gl\\/.*'],\n",
    "                                                       'file_names': ['mit_da.txt', 'mit_en,txt', 'mit_kl.txt']},\n",
    "               \n",
    "                'https://aul.gl':                     {'lang_codes': ['/kl/in-gr-oplev-kalaallit-nunaat/', '/da/oplev-groenland/', '/en/experience-greenland/'],\n",
    "                                                       'regex_keys': [r'\\/kl\\/in-gr-oplev-kalaallit-nunaat\\/.*', r'\\/da\\/oplev-groenland\\/.*', r'\\/en\\/experience-greenland\\/.*'],\n",
    "                                                       'file_names': ['aul_kl.txt', 'aul_da.txt', 'aul_en.txt']},\n",
    "               \n",
    "                'https://www.kni.gl':                 {'lang_codes': ['/kl/', '/da/', '/en/'], \n",
    "                                                       'regex_keys': [r'\\/kl\\/.*', r'\\/da\\/.*', r'\\/en\\/.*'],\n",
    "                                                       'file_names': ['kni_kl.txt', 'kni_da.txt', 'kni_en.txt']},\n",
    "               \n",
    "                'https://nukissiorfiit.gl':           {'lang_codes': ['/kl/', '/da/'], \n",
    "                                                       'regex_keys': [r'\\/kl\\/.*', r'\\/da\\/.*'],\n",
    "                                                       'file_names': ['nuki_kl.txt', 'nuki_da.txt']},\n",
    "               \n",
    "                'https://nunaoil.gl':                 {'lang_codes': ['/kl/', '', '/en/'],\n",
    "                                                       'regex_keys': [r'\\/kl\\/.*', r'\\/en\\/.*'],\n",
    "                                                       'file_names': ['nuna_kl.txt', 'nuna_da.txt', 'nuna_en.txt']},\n",
    "               \n",
    "                'https://nanoqmedia.gl':              {'lang_codes': ['/kl/', '/da/'], \n",
    "                                                       'regex_keys': [r'\\/kl\\/.*', r'\\/da\\/.*'],\n",
    "                                                       'file_names': ['nanoq_kl.txt', 'nanoq_da.txt']},\n",
    "               \n",
    "                'https://bus.gl':                     {'lang_codes': ['/kl/', '/da/', '/en/'], \n",
    "                                                       'regex_keys': [r'\\/kl\\/.*', r'\\/da\\/.*', r'\\/en\\/.*'],\n",
    "                                                       'file_names': ['bus_kl.txt', 'bus_da.txt', 'bus_en.txt']},\n",
    "               \n",
    "                'https://www.pilersuisoq.gl':         {'lang_codes': ['/kl/', '/da/'], \n",
    "                                                       'regex_keys': [r'\\/kl\\/.*', r'\\/da\\/.*'],\n",
    "                                                       'file_names': ['piler_kl.txt', 'piler_da.txt']},\n",
    "               \n",
    "                'https://www.pisiffik.gl':            {'lang_codes': ['/gl/', '/da/'],\n",
    "                                                       'regex_keys': [r'\\/gl\\/.*', r'\\/da\\/.*'],\n",
    "                                                       'file_names': ['pisiff_kl.txt', 'pisiff_da.txt']},\n",
    "               \n",
    "                'https://hotelarctic.com':            {'lang_codes': ['/gl/', '', '/en/'], \n",
    "                                                       'regex_keys': [r'\\/gl\\/.*', r'\\/.*', r'\\/en\\/.*'],\n",
    "                                                       'file_names': ['arctic_kl.txt', 'arctic_da.txt', 'arctic_en.txt']},\n",
    "               \n",
    "                'https://hhe.gl':                     {'lang_codes': ['/?lang=gl', '', '/?lang=en'], \n",
    "                                                       'regex_keys': [r'\\/\\?lang=gl\\/.*', r'\\/.*', r'\\/\\?lang=en\\/.*'],\n",
    "                                                       'file_names': ['hhe_kl.txt', 'hhe_da.txt', 'hhe_en.txt']},\n",
    "               \n",
    "                'https://www.banknordik.gl':          {'lang_codes': ['/kl', '/da/'], \n",
    "                                                       'regex_keys': [r'\\/kl\\/.*', r'\\/da\\/.*'],\n",
    "                                                       'file_names': ['banknord_kl.txt', 'banknord_da.txt']},\n",
    "               \n",
    "                'https://www.polarseafood.gl':        {'lang_codes': [''], \n",
    "                                                       'regex_keys': [r'\\/.*'],\n",
    "                                                       'file_names': ['psf_kl.txt']},\n",
    "               \n",
    "                'https://www.polarseafood.dk':        {'lang_codes': [''], \n",
    "                                                       'regex_keys': [r'\\/.*'],\n",
    "                                                       'file_names': ['psf_da.txt']},\n",
    "               \n",
    "                'http://climategreenland.gl':         {'lang_codes': ['', '/da/', '/en/'], \n",
    "                                                       'regex_keys': [r'\\/.*', r'\\/da\\/.*', r'\\/en\\/.*'],\n",
    "                                                       'file_names': ['climate_kl.txt', 'climate_da.txt', 'climate_en.txt']},\n",
    "               \n",
    "                'https://www.businessingreenland.gl': {'lang_codes': ['/kl-GL', '/da', '/en'], \n",
    "                                                       'regex_keys': [r'\\/kl-GL\\/.*', r'\\/da\\/.*', r'\\/en\\/.*'],\n",
    "                                                       'file_names': ['big_kl.txt', 'big_da.txt', 'big_en.txt']},\n",
    "               \n",
    "                'https://dk.usembassy.gov':           {'lang_codes': ['/kal/', '/da/', ''], \n",
    "                                                       'regex_keys': [r'\\/kal\\/.*', r'\\/da\\/.*', r'\\/.*'],\n",
    "                                                       'file_names': ['embassy_kl.txt', 'embassy_da.txt', 'embassy_en.txt']},\n",
    "               \n",
    "                'https://nka.gl':                     {'lang_codes': [''], \n",
    "                                                       'regex_keys': [r'\\/.*'],\n",
    "                                                       'file_names': ['nka_kl.txt']},\n",
    "               \n",
    "                'https://da.nka.gl':                  {'lang_codes': [''], \n",
    "                                                       'regex_keys': [r'\\/.*'],\n",
    "                                                       'file_names': ['nka_da.txt']},\n",
    "               \n",
    "                'https://en.nka.gl':                  {'lang_codes': [''], \n",
    "                                                       'regex_keys': [r'\\/.*'],\n",
    "                                                       'file_names': ['nka_en.txt']},\n",
    "               \n",
    "                'https://natur.gl':                   {'lang_codes': ['/?lang=kl', '', '/?lang=en'], \n",
    "                                                       'regex_keys': [r'\\/\\?lang=kl\\/.*', r'\\/.*', r'\\/\\?lang=en\\/.*'],\n",
    "                                                       'file_names': ['natur_kl.txt', 'natur_da.txt', 'natur_en.txt']},\n",
    "               \n",
    "                'https://oqaasileriffik.gl':          {'lang_codes': ['', '/da/', '/en/'], \n",
    "                                                       'regex_keys': [r'\\/.*', r'\\/da\\/.*', r'\\/en\\/.*'],\n",
    "                                                       'file_names': ['oqaas_kl.txt', 'oqaas_da.txt', 'oqaas_en.txt']},\n",
    "               \n",
    "                'https://www.katak.gl':               {'lang_codes': ['/kl', '/da', '/en'], \n",
    "                                                       'regex_keys': [r'\\/kl\\/.*', r'\\/da\\/.*', r'\\/en\\/.*'],\n",
    "                                                       'file_names': ['katak_kl.txt', 'katak_da.txt', 'katak_en.txt']}\n",
    "               \n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hm4XFI6d_Kmg",
    "outputId": "0cf2f570-a7eb-4ba3-b230-d982665d9eb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 25/70 subdomains left to scrape\n"
     ]
    }
   ],
   "source": [
    "NUM_SUBDOM = sum([len(domain_dict[dom]['lang_codes']) for dom in domain_dict])\n",
    "print(f'We have {NUM_SUBDOM}/70 subdomains left to scrape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "e4ZuLLoNx5NM"
   },
   "outputs": [],
   "source": [
    "def getHTML(url):\n",
    "  '''\n",
    "  Purpose: Get HTML from input URL\n",
    "  \n",
    "  Args: \n",
    "  url -- web link\n",
    "  \n",
    "  Returns:\n",
    "  htmlstr (str) -- page HTML\n",
    "  soup (bs4 object) -- a BeautifulSoup object containing HTML properties\n",
    "  '''\n",
    "  my_session = requests.session()\n",
    "  for_cookies = my_session.get(DOMAIN_NAME)\n",
    "  cookies = for_cookies.cookies\n",
    "  headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:57.0) Gecko/20100101 Firefox/57.0'}\n",
    "  response = my_session.get(url, headers=headers, cookies=cookies)\n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "  htmlstr = soup.prettify()\n",
    "  return htmlstr, soup\n",
    "\n",
    "def getPageLinks(soup, html):\n",
    "   '''\n",
    "   Purpose: Gets page links from HTML\n",
    "\n",
    "   Args: \n",
    "   soup (bs4 object) -- a BeautifulSoup object containing HTML properties\n",
    "   html (str) -- an HTML string\n",
    "\n",
    "   Returns:\n",
    "   page_links (List[str]) -- a list of links found on page\n",
    "   '''\n",
    "    # Uncomment the following two lines and comment out the third if crawling from sermitsiaq.ag\n",
    "#     page_links = re.findall(REL_LINK_REGEX, html)\n",
    "#     page_links = [re.sub(r' .*', '', page.replace('>', '').replace('\"', '')) for page in page_links]\n",
    "    page_links = [link.get('href') for link in soup.find_all('a') if link.get('href') and str(link.get('href'))[0]=='/' and (str(link.get('href')).split('/')[1] not in BAD_SUBDOMS)] #and str(link.get('href'))[0:3]!='/kl']\n",
    "    return page_links\n",
    "\n",
    "def getPageText(soup):\n",
    "  '''\n",
    "  Purpose: Get plaintext from page\n",
    "  \n",
    "  Args:\n",
    "  soup (bs4 object) -- a BeautifulSoup object containing HTML properties\n",
    "  \n",
    "  Returns:\n",
    "  cleanTexts (List[str]) -- a list of cleaned texts from page\n",
    "  '''\n",
    "  global seen_sentences\n",
    "  # Extract plaintext from html\n",
    "  for script in soup([\"script\", \"style\"]):\n",
    "      script.extract()\n",
    "  text = soup.get_text()\n",
    "  lines = (line.strip() for line in text.splitlines())\n",
    "  chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "  text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "  sentences = text.split('.')\n",
    "  # Clean texts\n",
    "  cleanTexts = [clean(text) for text in sentences if text]\n",
    "  def customTextCheck(text: str) -> bool:\n",
    "    return (('\\n' not in set(text)) and not (re.search(r'https', text)) and (len(text.split())>1))\n",
    "  cleanTexts = [text+'\\n' for text in cleanTexts if customTextCheck(text)]\n",
    "  cleanTexts = [text for text in cleanTexts if text not in seen_sentences]\n",
    "  seen_sentences = seen_sentences.union(set(cleanTexts))\n",
    "  return cleanTexts\n",
    "\n",
    "def write_to_file(sentences, path_to_write):\n",
    "  with open(path_to_write, 'a') as f:\n",
    "    f.writelines(sentences)\n",
    "  \n",
    "def scrapeText(url, path_to_write):\n",
    "  '''\n",
    "  Wrapper function for entire crawling process. Feed it a homepage URL and the path to the directory\n",
    "  you want the crawled sentences to be written to. Then be patient!\n",
    "  '''\n",
    "  html, soup = getHTML(url)\n",
    "  text = getPageText(soup)\n",
    "  write_to_file(text, path_to_write)\n",
    "  page_links = getPageLinks(soup, html)\n",
    "  if (len(visited) % 10 == 0) and (len(visited) > 1):\n",
    "    print(f'Parsed {len(visited)} articles')\n",
    "  for rel_link in page_links:\n",
    "    if rel_link not in visited:\n",
    "      full_link = DOMAIN_NAME+rel_link\n",
    "      visited.add(rel_link)\n",
    "      #print(f'Currently scraping {full_link}')\n",
    "      scrapeText(full_link, path_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 10 articles\n",
      "Parsed 20 articles\n",
      "Parsed 30 articles\n",
      "Parsed 40 articles\n",
      "Parsed 50 articles\n",
      "Parsed 60 articles\n",
      "Parsed 70 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 80 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 90 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not crawl https://www.banknordik.gl/kl\n",
      "Parsed 100 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 110 articles\n",
      "Parsed 120 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "  8%|▊         | 1/12 [3:03:43<33:40:54, 11023.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not crawl https://www.banknordik.gl/da/\n",
      "Parsed 10 articles\n",
      "Parsed 20 articles\n",
      "Parsed 30 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 40 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 50 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      " 17%|█▋        | 2/12 [3:05:28<21:31:18, 7747.89s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not crawl https://www.polarseafood.gl\n",
      "Parsed 10 articles\n",
      "Parsed 20 articles\n",
      "Parsed 30 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 40 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      " 25%|██▌       | 3/12 [3:06:38<13:36:40, 5444.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not crawl https://www.polarseafood.dk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 10 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not crawl http://climategreenland.gl/da/\n",
      "Parsed 20 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 4/12 [3:08:05<8:31:37, 3837.19s/it] Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 10 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 20 articles\n",
      "Parsed 30 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 40 articles\n",
      "Could not crawl https://www.businessingreenland.gl/kl-GL\n",
      "Parsed 50 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 60 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 70 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 80 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not crawl https://www.businessingreenland.gl/da\n",
      "Parsed 90 articles\n",
      "Parsed 100 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      " 42%|████▏     | 5/12 [3:11:15<5:20:01, 2743.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not crawl https://www.businessingreenland.gl/en\n",
      "Parsed 10 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 10/12 [3:11:57<15:31, 465.82s/it]  Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      " 92%|█████████▏| 11/12 [3:12:18<05:32, 332.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 10 articles\n",
      "Parsed 20 articles\n",
      "Parsed 30 articles\n",
      "Parsed 40 articles\n",
      "Parsed 50 articles\n",
      "Parsed 60 articles\n",
      "Could not crawl https://www.katak.gl/kl\n",
      "Parsed 70 articles\n",
      "Parsed 80 articles\n",
      "Parsed 90 articles\n",
      "Parsed 100 articles\n",
      "Parsed 110 articles\n",
      "Parsed 120 articles\n",
      "Could not crawl https://www.katak.gl/da\n",
      "Parsed 130 articles\n",
      "Parsed 140 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [3:15:49<00:00, 979.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# Crawl on Greenlandic domains!!\n",
    "# BE WARNED: This'll take a LONG TIME if you're looking to crawl every domain in the domain dict\n",
    "\n",
    "for domain in tqdm(domain_dict):\n",
    "    visited = set() # visited links\n",
    "    seen_sentences = set()\n",
    "    DOMAIN_NAME = domain\n",
    "    subdict = domain_dict[domain]\n",
    "    subdoms, regexes, file_names = subdict['lang_codes'], subdict['regex_keys'], subdict['file_names']\n",
    "    \n",
    "    for subdom, regex, file_name in zip(subdoms, \n",
    "                                        regexes, \n",
    "                                        file_names):\n",
    "        HOMEPAGE = DOMAIN_NAME + subdom\n",
    "        REL_LINK_REGEX = regex\n",
    "        FILE_TO_WRITE = '../data/' + file_name\n",
    "        BAD_SUBDOMS = set(subdoms) - set(subdom)\n",
    "        try:\n",
    "            scrapeText(HOMEPAGE, FILE_TO_WRITE)\n",
    "        except:\n",
    "            print(f'Could not crawl {HOMEPAGE}')\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ling28_finalProj_web_crawling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
