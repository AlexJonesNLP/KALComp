{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Alex Jones (alexander.g.jones.23@dartmouth.edu) \\\\\n",
        "March 15, 2022 \\\\\n",
        "LING 28 (Rolando Coto-Solano), Winter 2022 \\\\\n",
        "Final Project\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This notebook contains code for finetuning and evaluating a [Kalaallisut-English NMT model](https://huggingface.co/Helsinki-NLP/opus-mt-kl-en/tree/112da788d18d56b8ac0699d57c4b087c919d1fe6) from Hugging Face."
      ],
      "metadata": {
        "id": "trMLMaiTuOo7"
      },
      "id": "trMLMaiTuOo7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02434795",
      "metadata": {
        "scrolled": true,
        "id": "02434795"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers[sentencepiece]\n",
        "!pip install bleu datasets\n",
        "from bleu import list_bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, MarianMTModel\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from datasets import load_metric, DatasetDict, load_dataset\n",
        "import json\n",
        "from google.colab import drive\n",
        "import os\n",
        "os.environ['WANDDB_DISABLED']='true' "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "5QWHfy3Pu93v"
      },
      "id": "5QWHfy3Pu93v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dce75961",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dce75961",
        "outputId": "ee140e1c-51da-4988-b70a-dfa7a58d0fea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    \n",
        "    if torch.cuda.get_device_name(0) == \"Tesla K40m\":\n",
        "        raise GPUError(\"GPU Error: No compatible GPU found\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94887672",
      "metadata": {
        "id": "94887672"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'Helsinki-NLP/opus-mt-kl-en'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b5eece8",
      "metadata": {
        "id": "0b5eece8"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "!pip install sentencepiece\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7984d39b",
      "metadata": {
        "id": "7984d39b"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f574e5d",
      "metadata": {
        "id": "0f574e5d"
      },
      "outputs": [],
      "source": [
        "# Put model on GPU\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c75ff78b",
      "metadata": {
        "id": "c75ff78b"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset('json', data_files='/content/drive/MyDrive/ling28_final_proj/corpus.json', field='field')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "919e446a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "919e446a",
        "outputId": "a9f4577a-b77b-44ee-dc52-7811ab6aca90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 6393\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc8e5c8b",
      "metadata": {
        "id": "cc8e5c8b"
      },
      "outputs": [],
      "source": [
        "train_testvalid = dataset['train'].train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fbdd860",
      "metadata": {
        "id": "5fbdd860"
      },
      "outputs": [],
      "source": [
        "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97d4aa3b",
      "metadata": {
        "id": "97d4aa3b"
      },
      "outputs": [],
      "source": [
        "data = DatasetDict({\n",
        "    'train': train_testvalid['train'],\n",
        "    'test': test_valid['test'],\n",
        "    'valid': test_valid['train']})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f1416dc",
      "metadata": {
        "id": "4f1416dc"
      },
      "outputs": [],
      "source": [
        "def preprocess(data,\n",
        "               prefix,\n",
        "               max_input_length,\n",
        "               max_target_length,\n",
        "               source_lang,\n",
        "               target_lang):\n",
        "  '''\n",
        "  Tokenize and reorganize train/val/test data\n",
        "  '''\n",
        "  inputs = [prefix + ex[source_lang] for ex in data[\"translation\"]] #[prefix + ex for ex in data[\"kl\"]]\n",
        "  targets = [ex[target_lang] for ex in data[\"translation\"]] #[ex for ex in data[\"en\"]]\n",
        "  model_inputs = tokenizer(inputs, padding=True, truncation=True)\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "      labels = tokenizer(targets, padding=True, truncation=True)\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    \n",
        "  return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab53cacb",
      "metadata": {
        "id": "ab53cacb"
      },
      "outputs": [],
      "source": [
        "# Some constants for preprocessing\n",
        "PREFIX = ''\n",
        "MAX_INPUT_LENGTH = MAX_TARGET_LENGTH = 128\n",
        "SRC_LANG = 'kl'\n",
        "TGT_LANG = 'en'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf53dba",
      "metadata": {
        "id": "0cf53dba"
      },
      "outputs": [],
      "source": [
        "# Preprocess entire dataset\n",
        "custom_preprocess = lambda data: preprocess(data,\n",
        "                                            PREFIX,\n",
        "                                            MAX_INPUT_LENGTH,\n",
        "                                            MAX_TARGET_LENGTH,\n",
        "                                            SRC_LANG,\n",
        "                                            TGT_LANG)\n",
        "tokenized_data = data.map(custom_preprocess, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b5d41d1",
      "metadata": {
        "id": "4b5d41d1"
      },
      "outputs": [],
      "source": [
        "tokenized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f99ba14",
      "metadata": {
        "id": "2f99ba14"
      },
      "outputs": [],
      "source": [
        "# Define the finetuning hyperparameters\n",
        "BATCH_SIZE = 8\n",
        "MODEL_NAME = MODEL_NAME.split('/')[-1]\n",
        "EVAL_STRATEGY = 'epoch'\n",
        "LR = 2e-5 # learning rate\n",
        "WEIGHT_DECAY = 0.01\n",
        "SAVE_LIMIT = 3\n",
        "TRAIN_EPOCHS = 1\n",
        "PRED_GEN_FLAG = True\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{MODEL_NAME}-finetuned-{SRC_LANG}-to-{TGT_LANG}\",\n",
        "    evaluation_strategy = EVAL_STRATEGY,\n",
        "    learning_rate=LR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    save_total_limit=SAVE_LIMIT,\n",
        "    num_train_epochs=TRAIN_EPOCHS,\n",
        "    predict_with_generate=PRED_GEN_FLAG   \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d533b2b8",
      "metadata": {
        "id": "d533b2b8"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa5d191f",
      "metadata": {
        "id": "aa5d191f"
      },
      "outputs": [],
      "source": [
        "# Functions for computing metrics from model predictions\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81a702d0",
      "metadata": {
        "id": "81a702d0"
      },
      "outputs": [],
      "source": [
        "# Instantiate model trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"valid\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "148d03c7",
      "metadata": {
        "scrolled": true,
        "id": "148d03c7"
      },
      "outputs": [],
      "source": [
        "# Finetune model!\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate model"
      ],
      "metadata": {
        "id": "TltRvKHsvmI3"
      },
      "id": "TltRvKHsvmI3"
    },
    {
      "cell_type": "code",
      "source": [
        "FINETUNED_MODEL = '/content/opus-mt-kl-en-finetuned-kl-to-en/checkpoint-500'\n",
        "finetuned_model = MarianMTModel.from_pretrained(FINETUNED_MODEL)"
      ],
      "metadata": {
        "id": "NpRBK_VnRHQs"
      },
      "id": "NpRBK_VnRHQs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = open('/content/drive/MyDrive/ling28_final_proj/test_set.txt', 'r').readlines()"
      ],
      "metadata": {
        "id": "uKG7XC6FR7gk"
      },
      "id": "uKG7XC6FR7gk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tuples = []\n",
        "tup = []\n",
        "for sent in test_set:\n",
        "  if sent == '\\n':\n",
        "    test_tuples.append(tup)\n",
        "    tup = []\n",
        "  else:\n",
        "    tup.append(sent)"
      ],
      "metadata": {
        "id": "YBBnha1RTd_A"
      },
      "id": "YBBnha1RTd_A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kl_sents = [tup[0] for tup in test_tuples if len(tup)>1]\n",
        "en_sents_gold = [tup[1] for tup in test_tuples if len(tup)>1]"
      ],
      "metadata": {
        "id": "5KpokZ67TxhB"
      },
      "id": "5KpokZ67TxhB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(en_sents_gold)==len(kl_sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8KnOmA3TGEi",
        "outputId": "9a2845c8-0d28-471a-b507-c4225bb52fe1"
      },
      "id": "x8KnOmA3TGEi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model.cuda() # Put model on GPU"
      ],
      "metadata": {
        "id": "IB-XmvAAUwxM"
      },
      "id": "IB-XmvAAUwxM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translateKLtoEN(sentences,\n",
        "                    tokenizer,\n",
        "                    model,\n",
        "                    device):\n",
        "    \n",
        "    tokenized = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
        "    tokenized.to(device)\n",
        "    translated = model.generate(**tokenized)\n",
        "    decoded = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "    tokenized.to('cpu')\n",
        "    return decoded"
      ],
      "metadata": {
        "id": "Bd4vHWUmVB4R"
      },
      "id": "Bd4vHWUmVB4R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SENTS = len(kl_sents)\n",
        "BATCH_SIZE = 8\n",
        "NUM_BATCHES = (NUM_SENTS // BATCH_SIZE) + 1\n",
        "print(f'We will translate {NUM_BATCHES} batches of size {BATCH_SIZE}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK3Ha6rBVFvc",
        "outputId": "ae80f83b-6653-4b50-fac1-1a40aef346dc"
      },
      "id": "yK3Ha6rBVFvc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We will translate 615 batches of size 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "transl_kl_sents = []\n",
        "start = time.time()\n",
        "for i in range(NUM_BATCHES):\n",
        "    transl_kl_sents.extend(translateKLtoEN(kl_sents[i*BATCH_SIZE : (i+1)*BATCH_SIZE],\n",
        "                                           tokenizer,\n",
        "                                           model,\n",
        "                                           device))\n",
        "    print(\"Completed batch {:} of {:}\".format(i+1, NUM_BATCHES))\n",
        "end = time.time()\n",
        "print(\"Time taken: {:.3f}\".format(end-start))\n",
        "open('/content/drive/MyDrive/ling28_final_proj/en_preds.txt', 'w').writelines(transl_kl_sents)"
      ],
      "metadata": {
        "id": "BAZd-_cJVJ3f"
      },
      "id": "BAZd-_cJVJ3f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getBLEU(s1, s2):\n",
        "  return sentence_bleu([s1.split()], s2.split(), smoothing_function=SmoothingFunction().method4)*100"
      ],
      "metadata": {
        "id": "cHolmVbJXbpZ"
      },
      "id": "cHolmVbJXbpZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_scores = []\n",
        "for s1,s2 in zip(en_sents_gold, transl_kl_sents):\n",
        "  try:\n",
        "    bleu_scores.append(getBLEU(s1, s2))\n",
        "  except:\n",
        "    continue"
      ],
      "metadata": {
        "id": "77zd5EXBX99N"
      },
      "id": "77zd5EXBX99N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Average BLEU score: {sum(bleu_scores) / len(bleu_scores)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj3seAggYylr",
        "outputId": "bf6bb81f-58cb-49d8-d5f7-bcc4c8c07651"
      },
      "id": "yj3seAggYylr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU score: 27.753683440550986\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "finetune.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}