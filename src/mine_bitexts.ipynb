{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a9af712",
   "metadata": {},
   "source": [
    "Alex Jones (alexander.g.jones.23@dartmouth.edu) <br>\n",
    "March 15, 2022 <br>\n",
    "LING 28 (Rolando Coto-Solano), Winter 2022 <br>\n",
    "Final Project\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains code for mining pseudoparallel sentence pairs between Kalaallisut and Danish monolingual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee1ef8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers[sentencepiece] faiss-gpu\n",
    "!pip install -U sentence-transformers\n",
    "import torch\n",
    "from torch import nn\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import faiss\n",
    "import youtokentome as yttm\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0075022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    if torch.cuda.get_device_name(0) == \"Tesla K40m\":\n",
    "        raise GPUError(\"GPU Error: No compatible GPU found\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eed14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LaBSE'\n",
    "sentence_model = SentenceTransformer(model_name)\n",
    "sentence_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c19fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the GPU with Faiss\n",
    "GPU = faiss.StandardGpuResources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a8b59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Params\n",
    "******\n",
    "src_emb: array of size number_of_source_sentences X embedding_dimension\n",
    "tgt_emb: array of size number_of_target_sentences X embedding_dimension\n",
    "k: number of neighbors to return\n",
    "batch_size: batch size\n",
    "\n",
    "Returns\n",
    "*******\n",
    "cos_sims: cosine similarity scores for each of k nearest neighbors for each source sentence\n",
    "inds: target indices of k nearest neighbors for each source sentence\n",
    "\n",
    "Modeled off of LASER source code: https://github.com/facebookresearch/LASER/blob/master/source/mine_bitexts.py\n",
    "\n",
    "'''\n",
    "\n",
    "def knnSearch(src_emb, tgt_emb, k=1, batch_size=1):\n",
    "    emb_dim = src_emb.shape[1] # Embedding dimension\n",
    "    num_src_sents = src_emb.shape[0]\n",
    "    num_tgt_sents = tgt_emb.shape[0]\n",
    "    cos_sims = np.zeros((num_src_sents, k), dtype=np.float32)\n",
    "    inds = np.zeros((num_src_sents, k), dtype=np.int64)\n",
    "    for s_min in tqdm(range(0, num_src_sents, batch_size)):\n",
    "        s_max = min(s_min + batch_size, num_src_sents)\n",
    "        src_sims = []\n",
    "        src_inds = []\n",
    "        for t_min in range(0, num_tgt_sents, batch_size):\n",
    "            t_max = min(t_min + batch_size, num_tgt_sents)\n",
    "            idx = faiss.IndexFlatIP(emb_dim)\n",
    "            idx = faiss.index_cpu_to_gpu(GPU, 0, idx)\n",
    "            idx.add(tgt_emb[t_min : t_max])\n",
    "            src_sim, src_ind = idx.search(src_emb[s_min : s_max], min(k, t_max-t_min))\n",
    "            src_sims.append(src_sim)\n",
    "            src_inds.append(src_ind + t_min)\n",
    "            del idx\n",
    "        src_sims = np.concatenate(src_sims, axis=1)\n",
    "        src_inds = np.concatenate(src_inds, axis=1)\n",
    "        sorted_inds = np.argsort(-src_sims, axis=1)\n",
    "        for i in range(s_min, s_max):\n",
    "            for j in range(k):\n",
    "                cos_sims[i, j] = src_sims[i-s_min, sorted_inds[i-s_min, j]]\n",
    "                inds[i, j] = src_inds[i-s_min, sorted_inds[i-s_min, j]]\n",
    "    return cos_sims, inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fee1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves k-nearest neighbor indices and similarity means for margin scoring\n",
    "# If forward: finds neearest neighbors and indices for all source sentences\n",
    "# If backward: finds nearest neighbors and indices for all target sentences\n",
    "# In the approach implemented in our paper, we perform both forward and backward search\n",
    "\n",
    "def directedMeansAndInds(src_emb, tgt_emb, forward=False, backward=False, k=1, batch_size=1):\n",
    "    assert forward != backward, \"Please choose either forward or backward\"\n",
    "    if forward:\n",
    "        cos_sims, inds = knnSearch(src_emb, tgt_emb, min(tgt_emb.shape[0], k), batch_size)\n",
    "        return cos_sims.mean(axis=1), inds\n",
    "    elif backward:\n",
    "        cos_sims, inds = knnSearch(tgt_emb, src_emb, min(src_emb.shape[0], k), batch_size)\n",
    "        return cos_sims.mean(axis=1), inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1604f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Params\n",
    "******\n",
    "src_embs: array of size number_of_source_sentences X embedding_dimension\n",
    "tgt_embs: array of size number_of_source_sentences X embedding_dimension\n",
    "batch_size: batch size\n",
    "num_neighbors: number of neighbors\n",
    "\n",
    "Returns\n",
    "*******\n",
    "concat_pairs: list of mined sentence pairs\n",
    "margin_scores: list of scores corresponding to mined pairs\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def mineSentencePairs(src_embs, tgt_embs, batch_size=100, num_neighbors=4):\n",
    "\n",
    "    # Retrieve means and indices in the forward direction . . .\n",
    "    fwd_means, fwd_inds = directedMeansAndInds(src_embs, tgt_embs, forward=True, k=num_neighbors, batch_size=batch_size)\n",
    "    # . . . and in the backward direction\n",
    "    bwd_means, bwd_inds = directedMeansAndInds(src_embs, tgt_embs, backward=True, k=num_neighbors, batch_size=batch_size)\n",
    "\n",
    "    fwd_margin_scores = np.zeros(fwd_inds.shape)\n",
    "    for i in tqdm(range(fwd_inds.shape[0])):\n",
    "        for j in range(fwd_inds.shape[1]):\n",
    "            tgt_ind = fwd_inds[i,j]\n",
    "            # Compute ratio margin score between each source sentence and each of its k-nearest neighbors\n",
    "            margin_score = (src_embs[i].dot(tgt_embs[tgt_ind])) / np.average((fwd_means[i], bwd_means[tgt_ind]))\n",
    "            # Store the result\n",
    "            fwd_margin_scores[i,j] = margin_score\n",
    "    \n",
    "    # We will store the source index, target index, and margin score for the best\n",
    "    # pairs found using forward search\n",
    "    best = np.zeros((fwd_inds.shape[0], 3))\n",
    "    # Take pair that maximizes margin score for each source sentence\n",
    "    best_inds = fwd_inds[np.arange(src_embs.shape[0]), fwd_margin_scores.argmax(axis=1)]\n",
    "    for i in range(fwd_inds.shape[0]):\n",
    "        best_score, ind = (np.max(fwd_margin_scores[i]), np.argmax(fwd_margin_scores[i]))\n",
    "        best[i] = ((i+1, best_inds[i]+1, best_score)) # Assumption is that GROUND TRUTH VALUES ARE 1-INDEXED!!!\n",
    "\n",
    "    # Repeat process in backward direction (finding matches in source text for target sentences)\n",
    "    bwd_margin_scores = np.zeros(bwd_inds.shape)\n",
    "    for i in tqdm(range(bwd_inds.shape[0])):\n",
    "        for j in range(bwd_inds.shape[1]):\n",
    "            tgt_ind = bwd_inds[i,j]\n",
    "            margin_score = (tgt_embs[i].dot(src_embs[tgt_ind])) / np.average((bwd_means[i], fwd_means[tgt_ind]))\n",
    "            bwd_margin_scores[i,j] = margin_score\n",
    "            \n",
    "    bwd_best = np.zeros((bwd_inds.shape[0], 3))\n",
    "    best_inds = bwd_inds[np.arange(tgt_embs.shape[0]), bwd_margin_scores.argmax(axis=1)]\n",
    "    for i in range(bwd_inds.shape[0]):\n",
    "        best_score, ind = (np.max(bwd_margin_scores[i]), np.argmax(bwd_margin_scores[i]))\n",
    "        bwd_best[i] = ((best_inds[i]+1, i+1, best_score))\n",
    "    \n",
    "    # Best triples (src_idx, tgt_idx, margin_score) from forward/backward searches\n",
    "    fwd_best = [tuple(best[i]) for i in range(best.shape[0])]\n",
    "    bwd_best = [tuple(bwd_best[i]) for i in range(bwd_best.shape[0])]\n",
    "\n",
    "    pairs_and_scores = []\n",
    "    # Take UNION of forward and backward searches\n",
    "    pairs_and_scores = list(set(fwd_best) | set(bwd_best))\n",
    "\n",
    "    pairs_and_scores = list(dict.fromkeys(pairs_and_scores))\n",
    "    concat_pairs = [(triplet[0], triplet[1]) for triplet in pairs_and_scores] # Store indices only\n",
    "    concat_pairs_int = []\n",
    "    for tup in concat_pairs:\n",
    "        concat_pairs_int.append((int(tup[0]), int(tup[1]))) # Ground-truth indices are ints, so change type\n",
    "    concat_pairs = concat_pairs_int\n",
    "\n",
    "    margin_scores = [triplet[2] for triplet in pairs_and_scores] # Store scores only\n",
    "                                    \n",
    "    return concat_pairs, margin_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad644652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in monolingual sentences\n",
    "kl_sents = open('./data/kl/mono.kl', 'r').readlines()\n",
    "da_sents = open('./data/da/mono.da', 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e3676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BPE model on Kalaallisut data\n",
    "yttm.BPE.train(data='./data/kl/mono.kl', \n",
    "               vocab_size=10000, \n",
    "               model='../kl_bpe.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2850a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_model = yttm.BPE(model='../kl_bpe.model') # load BPE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33539766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBPETokens(sentences, bpe_model):\n",
    "    '''\n",
    "    Tokenize sentences using trained BPE model\n",
    "    '''\n",
    "    tokenized_sents = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        bpe_toks = bpe_model.encode(sentence, output_type=yttm.OutputType.SUBWORD)\n",
    "        just_toks = ''.join([tok+' ' for tok in bpe_toks]).replace('▁', '')\n",
    "        tokenized_sents.append(just_toks)\n",
    "    return tokenized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3074e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment Kalaallisut sentences using BPE\n",
    "kl_sents_seg = getBPETokens(kl_sents, bpe_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11934b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_embs = sentence_model.encode(kl_sents) # Embed sentences using LaBSE (this'll take a minute, be patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35cfc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_embs = sentence_model.encode(da_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a112efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pairs, scores = mineSentencePairs(kl_embs, da_embs) # Mine pseudoparallel sentences and keep margin scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a9671",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(scores)\n",
    "plt.xlabel('Margin score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcffdc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN_THRESHOLD = 1.2\n",
    "kl_sents_mined = [kl_sents[pair[0]-1] for pair in sent_pairs]\n",
    "da_sents_mined = [da_sents[pair[0]-1] for pair in sent_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f046a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mined_1_20 = [(kl_sents_mined[i], da_sents_mined[i]) for i in range(len(kl_sents_mined)) if scores[i] > MARGIN_THRESHOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ccfb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "mined_1_20 = [pair for pair in mined_1_20 if pair[0]!=pair[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d241ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mined {len(mined_1_20)} sentence pairs with a margin threshold of {MARGIN_THRESHOLD}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4376814",
   "metadata": {},
   "outputs": [],
   "source": [
    "mined_1_20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4681f4",
   "metadata": {},
   "source": [
    "### Trying dictionaries from https://www.mobileread.com/forums/showthread.php?t=20480&page=11\n",
    "### With both Kl-Da and Kl-En, we're about to translate $\\approx 16\\%$ of the words in the Kl corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b5a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_da_dict = pd.read_excel('./data/kl-da/kl-da_dict.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e4d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_da_dict.rename(columns={'Allattaasitaaq - New orthography': 'Kl', 'Qallunaatut - Danish': 'Da'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8511fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_da_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c0c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_da_dict = {kl:da for kl,da in zip(kl_da_dict['Kl'], kl_da_dict['Da'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1844aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate Kalaallisut words to Danish\n",
    "kl_sents_trans_da = []\n",
    "count = 0\n",
    "total = 0\n",
    "for sent in tqdm(kl_sents_seg):\n",
    "    trans_sent = ''\n",
    "    for word in sent.split():\n",
    "        if word in kl_da_dict:\n",
    "            transl = kl_da_dict[word]\n",
    "            count += 1\n",
    "        else:\n",
    "            transl = word\n",
    "        total += 1\n",
    "        trans_sent += word + ' '\n",
    "    kl_sents_trans_da.append(trans_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b54bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{round((count/total)*100, 2)}% of words were translated to Danish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_en_dict = {}\n",
    "with open('./data/en/kl-en_dict.txt', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for line in reader:\n",
    "        kl_en_dict[line[0]] = line[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a731313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate other Kalaallisut words to EEnglish\n",
    "kl_sents_trans_da_en = []\n",
    "for sent in tqdm(kl_sents_trans):\n",
    "    trans_sent = ''\n",
    "    for word in sent.split():\n",
    "        if word in kl_en_dict:\n",
    "            transl = kl_en_dict[word]\n",
    "            count += 1\n",
    "        else:\n",
    "            transl = word\n",
    "        trans_sent += word + ' '\n",
    "    kl_sents_trans_da_en.append(trans_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4084f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{round((count/total)*100, 2)}% of words were translated to English or Danish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eed762",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_embs_trans = sentence_model.encode(kl_sents_trans_da_en) # Embed the \"code-switched\" Kalaallisut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86012fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pairs, scores = mineSentencePairs(kl_embs_trans, da_embs) # Mine pseudoparallel seentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ba554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep sentence pairs above margin threshold\n",
    "MARGIN_THRESHOLD = 0\n",
    "kl_sents_mined = [kl_sents[pair[0]-1] for pair in sent_pairs]\n",
    "kl_sents_mined_trans = [kl_sents_trans_da_en[pair[0]-1] for pair in sent_pairs]\n",
    "da_sents_mined = [da_sents[pair[0]-1] for pair in sent_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a778e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mined_0 = [(kl_sents_mined_trans[i], da_sents_mined[i]) for i in range(len(kl_sents_mined)) if scores[i] > MARGIN_THRESHOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd5ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mined_0 = [pair for pair in mined_0 if pair[0]!=pair[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c252b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mined {len(mined_0)} sentence pairs with a margin threshold of {MARGIN_THRESHOLD}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7cfa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_sents_final = [pair[0] for pair in mined_1_04] # get final sentences for Kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7b6545",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_sents_final = [pair[1] for pair in mined_1_04] # get final sentences for Da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe085dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write final sentence pairs to file\n",
    "pd.DataFrame({'kl': kl_sents_final, 'da': da_sents_final}).to_csv('./data/kl-da/kl-da.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
