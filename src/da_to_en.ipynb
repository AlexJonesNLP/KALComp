{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bff1e233",
   "metadata": {},
   "source": [
    "Alex Jones (alexander.g.jones.23@dartmouth.edu) <br>\n",
    "March 15, 2022 <br>\n",
    "LING 28 (Rolando Coto-Solano), Winter 2022 <br>\n",
    "Final Project\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains code for translating mined Danish sentences to English using a pretrained MT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d026f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5da27589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA TITAN V\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    if torch.cuda.get_device_name(0) == \"Tesla K40m\":\n",
    "        raise GPUError(\"GPU Error: No compatible GPU found\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a22899",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'Helsinki-NLP/opus-mt-da-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10e83309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e168e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2286a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(58930, 512, padding_idx=58929)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(58930, 512, padding_idx=58929)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(58930, 512, padding_idx=58929)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=58930, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put model on GPU\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5417136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateDAtoEN(sentences,\n",
    "                    tokenizer,\n",
    "                    model,\n",
    "                    device):\n",
    "    '''\n",
    "    Translate Danish sentences to English using pretrained Hugging Face model\n",
    "    '''\n",
    "    tokenized = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "    tokenized.to(device)\n",
    "    translated = model.generate(**tokenized)\n",
    "    decoded = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "    tokenized.to('cpu')\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1097b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_da_corpus = pd.read_csv('./data/kl-da/kl-da.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dc55f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kl</th>\n",
       "      <th>da</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>netredaktionen ansvarshavend chefredaktørchris...</td>\n",
       "      <td>flere kritiser laksepolitikken sermitsiaq\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mary arctica angalaneq 144-mut allannguuteqart...</td>\n",
       "      <td>nægtede godkend lån siumut notat er udsendt ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>statistik sermitsiaq\\n</td>\n",
       "      <td>forsiden indland nuuk politik erhverv politi u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>polar aassik kivivoq sermitsiaq\\n</td>\n",
       "      <td>vi har ikk undersøgt det siger ida abelsenhans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dk iserfigiuk oqalliffimmut tikilluarit oqalli...</td>\n",
       "      <td>unikt sort hul opdaget mælkevejen sermitsiaq\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6388</th>\n",
       "      <td>\" www\\n</td>\n",
       "      <td>arbejd vore medarbejder på gøre leveranc klar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6389</th>\n",
       "      <td>filmi illoqarfiup filmertarfiani pingasunngorp...</td>\n",
       "      <td>når de kommend atlantlufthavn ilulissat og nuu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6390</th>\n",
       "      <td>bussit ingerlaarnissaannut pilersaarut faceboo...</td>\n",
       "      <td>grønland erhverv vil derfor på det kraftigst o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6391</th>\n",
       "      <td>euro-nngorlugit nusunneqartarput\\n</td>\n",
       "      <td>del på facebook del på twitter del på googl em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6392</th>\n",
       "      <td>københavn aqqussaaqqaanngikkaluarlugu steen ly...</td>\n",
       "      <td>der er dog én ting der nager ham\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6393 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     kl  \\\n",
       "0     netredaktionen ansvarshavend chefredaktørchris...   \n",
       "1     mary arctica angalaneq 144-mut allannguuteqart...   \n",
       "2                                statistik sermitsiaq\\n   \n",
       "3                     polar aassik kivivoq sermitsiaq\\n   \n",
       "4     dk iserfigiuk oqalliffimmut tikilluarit oqalli...   \n",
       "...                                                 ...   \n",
       "6388                                            \" www\\n   \n",
       "6389  filmi illoqarfiup filmertarfiani pingasunngorp...   \n",
       "6390  bussit ingerlaarnissaannut pilersaarut faceboo...   \n",
       "6391                 euro-nngorlugit nusunneqartarput\\n   \n",
       "6392  københavn aqqussaaqqaanngikkaluarlugu steen ly...   \n",
       "\n",
       "                                                     da  \n",
       "0           flere kritiser laksepolitikken sermitsiaq\\n  \n",
       "1     nægtede godkend lån siumut notat er udsendt ef...  \n",
       "2     forsiden indland nuuk politik erhverv politi u...  \n",
       "3     vi har ikk undersøgt det siger ida abelsenhans...  \n",
       "4        unikt sort hul opdaget mælkevejen sermitsiaq\\n  \n",
       "...                                                 ...  \n",
       "6388  arbejd vore medarbejder på gøre leveranc klar ...  \n",
       "6389  når de kommend atlantlufthavn ilulissat og nuu...  \n",
       "6390  grønland erhverv vil derfor på det kraftigst o...  \n",
       "6391  del på facebook del på twitter del på googl em...  \n",
       "6392                 der er dog én ting der nager ham\\n  \n",
       "\n",
       "[6393 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_da_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bee199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_sents = list(kl_da_corpus['da'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c3c601e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will translate 200 batches of size 32\n"
     ]
    }
   ],
   "source": [
    "NUM_SENTS = len(da_sents)\n",
    "BATCH_SIZE = 32\n",
    "NUM_BATCHES = (NUM_SENTS // BATCH_SIZE) + 1\n",
    "print(f'We will translate {NUM_BATCHES} batches of size {BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18a76dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch 1 of 200\n",
      "Completed batch 2 of 200\n",
      "Completed batch 3 of 200\n",
      "Completed batch 4 of 200\n",
      "Completed batch 5 of 200\n",
      "Completed batch 6 of 200\n",
      "Completed batch 7 of 200\n",
      "Completed batch 8 of 200\n",
      "Completed batch 9 of 200\n",
      "Completed batch 10 of 200\n",
      "Completed batch 11 of 200\n",
      "Completed batch 12 of 200\n",
      "Completed batch 13 of 200\n",
      "Completed batch 14 of 200\n",
      "Completed batch 15 of 200\n",
      "Completed batch 16 of 200\n",
      "Completed batch 17 of 200\n",
      "Completed batch 18 of 200\n",
      "Completed batch 19 of 200\n",
      "Completed batch 20 of 200\n",
      "Completed batch 21 of 200\n",
      "Completed batch 22 of 200\n",
      "Completed batch 23 of 200\n",
      "Completed batch 24 of 200\n",
      "Completed batch 25 of 200\n",
      "Completed batch 26 of 200\n",
      "Completed batch 27 of 200\n",
      "Completed batch 28 of 200\n",
      "Completed batch 29 of 200\n",
      "Completed batch 30 of 200\n",
      "Completed batch 31 of 200\n",
      "Completed batch 32 of 200\n",
      "Completed batch 33 of 200\n",
      "Completed batch 34 of 200\n",
      "Completed batch 35 of 200\n",
      "Completed batch 36 of 200\n",
      "Completed batch 37 of 200\n",
      "Completed batch 38 of 200\n",
      "Completed batch 39 of 200\n",
      "Completed batch 40 of 200\n",
      "Completed batch 41 of 200\n",
      "Completed batch 42 of 200\n",
      "Completed batch 43 of 200\n",
      "Completed batch 44 of 200\n",
      "Completed batch 45 of 200\n",
      "Completed batch 46 of 200\n",
      "Completed batch 47 of 200\n",
      "Completed batch 48 of 200\n",
      "Completed batch 49 of 200\n",
      "Completed batch 50 of 200\n",
      "Completed batch 51 of 200\n",
      "Completed batch 52 of 200\n",
      "Completed batch 53 of 200\n",
      "Completed batch 54 of 200\n",
      "Completed batch 55 of 200\n",
      "Completed batch 56 of 200\n",
      "Completed batch 57 of 200\n",
      "Completed batch 58 of 200\n",
      "Completed batch 59 of 200\n",
      "Completed batch 60 of 200\n",
      "Completed batch 61 of 200\n",
      "Completed batch 62 of 200\n",
      "Completed batch 63 of 200\n",
      "Completed batch 64 of 200\n",
      "Completed batch 65 of 200\n",
      "Completed batch 66 of 200\n",
      "Completed batch 67 of 200\n",
      "Completed batch 68 of 200\n",
      "Completed batch 69 of 200\n",
      "Completed batch 70 of 200\n",
      "Completed batch 71 of 200\n",
      "Completed batch 72 of 200\n",
      "Completed batch 73 of 200\n",
      "Completed batch 74 of 200\n",
      "Completed batch 75 of 200\n",
      "Completed batch 76 of 200\n",
      "Completed batch 77 of 200\n",
      "Completed batch 78 of 200\n",
      "Completed batch 79 of 200\n",
      "Completed batch 80 of 200\n",
      "Completed batch 81 of 200\n",
      "Completed batch 82 of 200\n",
      "Completed batch 83 of 200\n",
      "Completed batch 84 of 200\n",
      "Completed batch 85 of 200\n",
      "Completed batch 86 of 200\n",
      "Completed batch 87 of 200\n",
      "Completed batch 88 of 200\n",
      "Completed batch 89 of 200\n",
      "Completed batch 90 of 200\n",
      "Completed batch 91 of 200\n",
      "Completed batch 92 of 200\n",
      "Completed batch 93 of 200\n",
      "Completed batch 94 of 200\n",
      "Completed batch 95 of 200\n",
      "Completed batch 96 of 200\n",
      "Completed batch 97 of 200\n",
      "Completed batch 98 of 200\n",
      "Completed batch 99 of 200\n",
      "Completed batch 100 of 200\n",
      "Completed batch 101 of 200\n",
      "Completed batch 102 of 200\n",
      "Completed batch 103 of 200\n",
      "Completed batch 104 of 200\n",
      "Completed batch 105 of 200\n",
      "Completed batch 106 of 200\n",
      "Completed batch 107 of 200\n",
      "Completed batch 108 of 200\n",
      "Completed batch 109 of 200\n",
      "Completed batch 110 of 200\n",
      "Completed batch 111 of 200\n",
      "Completed batch 112 of 200\n",
      "Completed batch 113 of 200\n",
      "Completed batch 114 of 200\n",
      "Completed batch 115 of 200\n",
      "Completed batch 116 of 200\n",
      "Completed batch 117 of 200\n",
      "Completed batch 118 of 200\n",
      "Completed batch 119 of 200\n",
      "Completed batch 120 of 200\n",
      "Completed batch 121 of 200\n",
      "Completed batch 122 of 200\n",
      "Completed batch 123 of 200\n",
      "Completed batch 124 of 200\n",
      "Completed batch 125 of 200\n",
      "Completed batch 126 of 200\n",
      "Completed batch 127 of 200\n",
      "Completed batch 128 of 200\n",
      "Completed batch 129 of 200\n",
      "Completed batch 130 of 200\n",
      "Completed batch 131 of 200\n",
      "Completed batch 132 of 200\n",
      "Completed batch 133 of 200\n",
      "Completed batch 134 of 200\n",
      "Completed batch 135 of 200\n",
      "Completed batch 136 of 200\n",
      "Completed batch 137 of 200\n",
      "Completed batch 138 of 200\n",
      "Completed batch 139 of 200\n",
      "Completed batch 140 of 200\n",
      "Completed batch 141 of 200\n",
      "Completed batch 142 of 200\n",
      "Completed batch 143 of 200\n",
      "Completed batch 144 of 200\n",
      "Completed batch 145 of 200\n",
      "Completed batch 146 of 200\n",
      "Completed batch 147 of 200\n",
      "Completed batch 148 of 200\n",
      "Completed batch 149 of 200\n",
      "Completed batch 150 of 200\n",
      "Completed batch 151 of 200\n",
      "Completed batch 152 of 200\n",
      "Completed batch 153 of 200\n",
      "Completed batch 154 of 200\n",
      "Completed batch 155 of 200\n",
      "Completed batch 156 of 200\n",
      "Completed batch 157 of 200\n",
      "Completed batch 158 of 200\n",
      "Completed batch 159 of 200\n",
      "Completed batch 160 of 200\n",
      "Completed batch 161 of 200\n",
      "Completed batch 162 of 200\n",
      "Completed batch 163 of 200\n",
      "Completed batch 164 of 200\n",
      "Completed batch 165 of 200\n",
      "Completed batch 166 of 200\n",
      "Completed batch 167 of 200\n",
      "Completed batch 168 of 200\n",
      "Completed batch 169 of 200\n",
      "Completed batch 170 of 200\n",
      "Completed batch 171 of 200\n",
      "Completed batch 172 of 200\n",
      "Completed batch 173 of 200\n",
      "Completed batch 174 of 200\n",
      "Completed batch 175 of 200\n",
      "Completed batch 176 of 200\n",
      "Completed batch 177 of 200\n",
      "Completed batch 178 of 200\n",
      "Completed batch 179 of 200\n",
      "Completed batch 180 of 200\n",
      "Completed batch 181 of 200\n",
      "Completed batch 182 of 200\n",
      "Completed batch 183 of 200\n",
      "Completed batch 184 of 200\n",
      "Completed batch 185 of 200\n",
      "Completed batch 186 of 200\n",
      "Completed batch 187 of 200\n",
      "Completed batch 188 of 200\n",
      "Completed batch 189 of 200\n",
      "Completed batch 190 of 200\n",
      "Completed batch 191 of 200\n",
      "Completed batch 192 of 200\n",
      "Completed batch 193 of 200\n",
      "Completed batch 194 of 200\n",
      "Completed batch 195 of 200\n",
      "Completed batch 196 of 200\n",
      "Completed batch 197 of 200\n",
      "Completed batch 198 of 200\n",
      "Completed batch 199 of 200\n",
      "Completed batch 200 of 200\n",
      "Time taken: 1021.977\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "transl_da_sents = []\n",
    "start = time.time()\n",
    "for i in range(NUM_BATCHES):\n",
    "    transl_da_sents.extend(translateDAtoEN(da_sents[i*BATCH_SIZE : (i+1)*BATCH_SIZE],\n",
    "                                           tokenizer,\n",
    "                                           model,\n",
    "                                           device))\n",
    "    print(\"Completed batch {:} of {:}\".format(i+1, NUM_BATCHES))\n",
    "end = time.time()\n",
    "print(\"Time taken: {:.3f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d09aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'kl': kl_da_corpus['kl'], 'en': transl_da_sents}).to_csv('./data/kl-en/kl-en.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
